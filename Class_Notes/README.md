# Deep Learning in Computer Vision Class Note

# Introduction (week: 02/20)

## AI Branches

![jpg](images/img1.jpg)

**AI (Artificial Intelligence):**
The broadest term, AI refers to the simulation of human intelligence in machines, enabling them to perform tasks that typically require human cognition, such as reasoning, problem-solving, and decision-making.

**ML (Machine Learning):**
A subset of AI, ML involves training algorithms to learn patterns from data and make predictions or decisions without explicit programming. ML models improve performance with more data over time.

**DL (Deep Learning):** 
A specialized subset of ML, DL uses neural networks with multiple layers (deep neural networks) to process large amounts of data and perform complex tasks such as image recognition, natural language processing, and autonomous driving.

## Categories of Machine Learning

**Supervised learning:**
- label/ground-truth of data is given
- A type of machine learning where the model is trained on labeled data, mapping inputs to known outputs.

**Unsupervised learning:**
- label/ground-truth of data is unknown.
- A type of machine learning where the model finds patterns and structures in unlabeled data without predefined outputs.
- transforming the data into other representations.
- ex: image processing, e.g., local binary pattern  
clustering: e.g., k-means clustering

**Reinforcement learning:**
determine the optimal policy (i.e., the best
set of actions) based on the `reward` learn from the `environment`.

## Computer Vision
Make the computers understand images and videos. Identify relationships between objects in the image through image processing.

**More Applications: Vision in Space**  
Vision systems (JPL) used for several tasks:  
- Panorama stitching:  
‚ë† detect feature points in image pairs.  
‚ë° Find matching features in adjacent images.  
‚ë¢ Use a unified coordinate sys, to stitch the images together.
- 3D terrain modeling:  
‚ë† Detect feature points  
‚ë° obtain camera projection matrix.  
‚ë¢ project those feature points back to their 3D coordinates
- Obstacle detection, position tracking
- For more, read ‚ÄúComputer Vision on Mars‚Äù by Matthies et al.

![jpg](images/img2.jpg)

# Data Representation and Normalization (week: 02/24)
- Conventional ML : Use engineered (human selected) features to train model
- DL : Train model with raw data, the model will automatically extract features for you

## Data Representation
prior to training a model, you must determine:  
‚û¢ label ground-truth: eg. binary classification 0 or 1  
‚û¢ representation of input:  
> - raw data : DL
> - transform to other features : ML

## Normalization (Feature scaling)
**When:** Input features have different order of magnitudes  
**Why:** poor performance due to:  
‚û¢ Dominance of feature  <img src="images/img3.jpg" alt="Deeplearning AI" align="right" width="200">   
‚û¢ Numerical stability  
‚û¢ Convergence issues  

**Methods:**
-  Min-Max Normalization:  (normalize to 0 - 1)  
$x[:, j]_{normalization} = \frac{x[:, j] - min(x[:, j])}{max(x[:, j]) - min([:, j])}$    
    ‚û¢ Feature-wise scaling, across all the samples  
    ‚û¢ Most common scaling technique  
    ‚û¢ cons: think about inherent constraints in physics

- Mean normalization:
![jpg](images/img4.jpg)
- Z-score:
![jpg](images/img5.jpg)

**How to apply:**  
- Classification  
‚û¢ Usually no scaling in outputs.  
‚û¢ Need scaling in inputs.  
- Regression  
‚û¢ Usually need to scale in both inputs and outputs.  
‚û¢ If you do scaling while training the model, need scaling    during testing  

    **üîπTesting Data:**  
    1. apply feature transformation  
    2. scaling input & output using the same scaling factors   employed during training
    3. scale the predictions back to original values. 

    ![jpg](images/img6.jpg)



# Model evaluation (week: 02/27)

## Assumption in Machine Learning
- How you determine your training dataset so that your model works
on the unseen (test) dataset?  
    1. Training dataset must be ``representative enough``
    2. In other words, training and testing datasets are  `on the same distribution`  

    <img src="images/img7.jpg" width="250"/>

## Robustness Evaluation
**Goal:** check how bad your model could be when you change your selection of training and testing data set.  
1.  Do **repeated trials**! (Change the selected training and testing)
2.  show **statistics** of the performance of your model on
the **testing dataset**  ex: boxplot, normal distribution of multiple trials.

## Overvitting 
 **Overfitting:** model works well on training dataset, but performs poor on testing dataset.  
 - Blue dots: 2D feature vectors of men images  
 - Red dots: 2D feature vectors of women images  
 - Green curve: an overfitted model
 - Black curve: a more general model

 <img src="images/img8.jpg" width="150"/>  

 > *Detect Outliers: Suppose small portion of outliers => How to detect outliers?  
Once the model is trained appropriately it should be a general model. Feed the training samples into the model to make predictions. the samples with relatively larger
**"prediction error"** could be potential outliers.

### Reasons and solutions to Overfitting
1. Training dataset is not representative 

    ‚ûî re-select the training samples  

2.  Model complexity is high  

    ‚ûî reduce the model complexity  

3.  Train too much   

    ‚ûî reduce the number of training epochs

### When to stop Training
<img src="images/img9.jpg" width=""/> 

1. keep testing dataset for final evaluation
2. Stop when error(loss) on **validation dataset start to increase**   

(Overfit: accuracy of training dataset high validation low. Underfit:accuracy of training dataset low validation low )

## Confusion matrix and Accurracy
### **Example Interpretation of a ROC Figure & Using Confusion Matrix**

#### **Scenario: Fraud Detection Model**
We trained a **binary classification model** to detect fraudulent transactions. The **ROC curve and AUC score** help evaluate model performance, while the **confusion matrix** shows how predictions are classified.

---

### **1Ô∏è‚É£ Understanding the ROC Figure**
#### **Given Data:**
- **AUC = 0.87**
- The **ROC curve rises steeply** and approaches the top-left corner.

#### **Interpretation:**
- **AUC = 0.87** ‚Üí The model is **good at distinguishing fraud and non-fraud cases** (87% accuracy in ranking).
- The **curve is above the diagonal line**, meaning the model performs **better than random guessing**.
- A **higher threshold** (e.g., 0.8) reduces false positives but increases false negatives.
- A **lower threshold** (e.g., 0.3) catches more fraud cases but also increases false alarms.

üìå **If detecting fraud is critical**, we **lower the threshold** to **increase recall** (catching more fraudulent cases).

---

### **2Ô∏è‚É£ Confusion Matrix Interpretation**
The **confusion matrix** helps analyze classification errors at a chosen threshold.

#### **Confusion Matrix Example (Threshold = 0.5)**
| **Actual \ Predicted** | **Non-Fraud (0)** | **Fraud (1)** |
|------------------------|------------------|--------------|
| **Non-Fraud (0)**  | 900(TP) | 50(FN) |
| **Fraud (1)** (FN) | 30(FP)  | 20(TN) |

üìå **Key Metrics from the Confusion Matrix:**
- **True Positives (TP) = 20** ‚Üí Fraud correctly detected.
- **False Positives (FP) = 50** ‚Üí Non-fraud wrongly classified as fraud.
- **True Negatives (TN) = 900** ‚Üí Correct non-fraud classifications.
- **False Negatives (FN) = 30** ‚Üí Fraud cases **missed** by the model.

üìå **Key Insights:**
1. **Precision (TP / (TP + FP)) =** 20 / (20 + 50) = 0.29 (29%)
   - **Low precision** ‚Üí Many false positives (wrongly flagged transactions).
2. **Recall(TPR) (TP / (TP + FN)) =** 20 / (20 + 30) = 0.40 (40%)
   - **Low recall** ‚Üí Many fraud cases are missed.
3. **Accuracy = (TP + TN) / (Total Predictions)** = (20 + 900) / 1000 = 92%
   - **Accuracy is high, meaningless if dataset is highly unbalanced.** Might be misleading if fraud cases are rare.
4. **FPR = FP / (FP + TN)** = 30 / (20 + 30) = 60%
---

### **3Ô∏è‚É£ Adjusting the Threshold Using ROC**

- **Lower the threshold (e.g., 0.3)** ‚Üí Increases recall (fewer missed frauds) but may increase false positives.
- **Raise the threshold (e.g., 0.7)** ‚Üí Reduces false positives but **misses more fraud cases**.  

#### When to Prioritize High TPR(better) or Low FPR(better)?
**Trade-off:** high TPR often comes at the cost of a high FPR 
**Solution:** Combine high TPR with precision to avoid too many false positives.
| **Scenario** | **Prioritize High TPR (Low FN)?** | **Prioritize Low FPR (Low FP)?** |
|-------------|--------------------------------|------------------------------|
| **Medical Diagnosis (e.g., Cancer, COVID-19)** | ‚úÖ Yes (Missing a real case is dangerous) | ‚ùå No |
| **Spam Detection** | ‚úÖ Yes (Better to overfilter than miss spam) | ‚ùå No |
| **Fraud Detection** | ‚úÖ Yes (Better to block fraud than allow it) | ‚ùå No |
| **Airport Security Screening** | ‚úÖ Yes (Better safe than sorry) | ‚ùå No |
| **Hiring/Resume Screening** | ‚ùå No | ‚úÖ Yes (Avoid rejecting good candidates) |
| **Autonomous Vehicles (Self-Driving Car Stop System)** | ‚ùå No | ‚úÖ Yes (Avoid unnecessary stops) |
| **Criminal Investigations** | ‚úÖ Yes (Find suspects) | ‚úÖ Yes (Avoid accusing the wrong person) |

---

### **4Ô∏è‚É£ When to Use ROC vs. Confusion Matrix?**
| **Situation** | **Use ROC-AUC?** | **Use Confusion Matrix?** |
|--------------|-----------------|----------------------|
| Overall model performance | ‚úÖ Yes | ‚ùå No |
| Deciding the best threshold | ‚úÖ Yes | ‚úÖ Yes |
| Analyzing classification errors | ‚ùå No | ‚úÖ Yes |
| Imbalanced dataset | ‚ùå No (Use PR-AUC) | ‚úÖ Yes |

---



### **-Final Takeaways**
‚úî **ROC Curve** helps determine **optimal threshold trade-offs**.  
‚úî **AUC Score** summarizes model performance (**higher is better**).  
‚úî **Confusion Matrix** shows actual vs. predicted classifications.  
‚úî **Threshold tuning is necessary** to balance precision and recall.

